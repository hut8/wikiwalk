# wikiwalk

Shortest path algorithm between pages on Wikipedia. Let's say that you want to get from page `Exploding animal` to `Cucumber`. This thing will find the set of shortest paths between them, and return the path as a list of page titles.

## Prior Art

* [Six Degrees of Wikipedia](https://github.com/jwngr/sdow) by [Jacob Wenger](https://jwn.gr/) - The bi-directional BFS algorithm is based on this project. There are a lot of differences between the implementations: SDOW is in Python, and uses SQLite. This is in Rust and uses a bespoke graph memory-mapped database that I made. SDOW's import process uses shell scripts rather than a parser combinator library, and as a result, it is currently broken. Nonetheless, it's pretty brilliant,

## Graph Database

There are two files that comprise the adjacency list database of edges:

### vertex_al

Vertex Adjacency List

An array of records of `edges` in this format:

```none
null        ⇒ 0 as u32
vertex      ⇒ u32
vertexes    ⇒ vertex+
edges       ⇒ outgoing_vertexes, null, incoming_vertexes, null
```

To determine which vertex each record belongs to, see below (`vertex_al_ix`)

### vertex_al_ix

Vertex Adjacency List Index

An array of `u32` indexed by vertex ID. Each `u32` is the offset into `vertex_al` at which the edge data is located. So to load the record for the page with ID `1337`:

* Read the eight bytes at offset `1337 * 8` in `vertex_al_ix` ⇒ `offset_bytes`
* Decode `offset_bytes` into a `u64` ⇒ `offset`
* If `offset == 0`, the page either does not exist, or has no edges attached to it.
* Otherwise, read the record at `offset` in `vertex_al` (see above)

## Importing MySQL data for test

If you want to spot-check some data from MySQL, it's faster to import the dumps without indexes. First define the tables thusly:

```sql
CREATE TABLE `pagelinks` (
  `pl_from` int(8) unsigned NOT NULL DEFAULT 0,
  `pl_from_namespace` int(11) NOT NULL DEFAULT 0,
  `pl_target_id` bigint(20) unsigned NOT NULL
  ) ENGINE=InnoDB DEFAULT CHARSET=binary;

 CREATE TABLE `redirect` (
  `rd_from` int(8) unsigned NOT NULL DEFAULT 0,
  `rd_namespace` int(11) NOT NULL DEFAULT 0,
  `rd_title` varbinary(255) NOT NULL DEFAULT '',
  `rd_interwiki` varbinary(32) DEFAULT NULL,
  `rd_fragment` varbinary(255) DEFAULT NULL
  ) ENGINE=InnoDB DEFAULT CHARSET=binary ROW_FORMAT=COMPRESSED;

 CREATE TABLE `page` (
  `page_id` int(8) unsigned NOT NULL,
  `page_namespace` int(11) NOT NULL DEFAULT 0,
  `page_title` varbinary(255) NOT NULL DEFAULT '',
  `page_is_redirect` tinyint(1) unsigned NOT NULL DEFAULT 0,
  `page_is_new` tinyint(1) unsigned NOT NULL DEFAULT 0,
  `page_random` double unsigned NOT NULL DEFAULT 0,
  `page_touched` binary(14) NOT NULL,
  `page_links_updated` varbinary(14) DEFAULT NULL,
  `page_latest` int(8) unsigned NOT NULL DEFAULT 0,
  `page_len` int(8) unsigned NOT NULL DEFAULT 0,
  `page_content_model` varbinary(32) DEFAULT NULL,
  `page_lang` varbinary(35) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=binary;

CREATE TABLE `linktarget` (
  `lt_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `lt_namespace` int(11) NOT NULL,
  `lt_title` varbinary(255) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=binary;
```

Then read in dumps, skipping DDL

```sh
pv enwiki-*-pagelinks.sql | tail +34 | mysql wiki
pv enwiki-*-page.sql | tail +45 | mysql wiki
pv enwiki-*-redirect.sql | tail +38 | mysql wiki
pv enwiki-*-linktarget.sql | tail +34 | mysql wiki
```

Then build indexes

```sql
create index page_title_ix on page (page_title);
create index page_title_id on page(page_id);
create index page_title_namespace on page(page_namespace);
```

To export for later:

```sql
select * from page into outfile '/tmp/wiki-page-dump';
select * from pagelinks into outfile '/tmp/wiki-pagelinks-dump';
select * from redirect into outfile '/tmp/wiki-redirect-dump';
select * from linktarget into outfile '/tmp/wiki-linktarget-dump';
```

Then compress and such:

```sh
sudo mv /tmp/wiki-*-dump ~/data
sudo chown $(id -u):$(id -g) ~/data/wiki-*-dump
zstd -T0 ~/data/wiki-*-dump
```

Then to import (assuming wiki-page-dump is on the server at some location):

```sql
LOAD DATA INFILE 'wiki-page-dump' INTO TABLE page;
LOAD DATA INFILE 'wiki-pagelinks-dump' INTO TABLE pagelinks;
LOAD DATA INFILE 'wiki-redirect-dump' INTO TABLE redirect;
LOAD DATA INFILE 'wiki-linktarget-dump' INTO TABLE linktarget;
```

## SQLite Databases

There are two SQLite databases:

* master.db - this is the persistent database with an unlimited
  lifetime. It is used to log performance and usage.
* graph.db - this is generated by the build routine, and also contains
  a cache. The cache is here because it becomes useless when the rest
  of the graph is updated.

## Queries

To find large paths:

``` sql
SELECT
  sv.title AS 'source_title',
  tv.title AS 'target_title',
  coalesce(json_array_length(json_extract(path_data, '$[0]')),0) AS degrees
  FROM path p
  INNER JOIN vertexes sv ON p.source_page_id=sv.id
  INNER JOIN vertexes tv ON p.target_page_id=tv.id
  WHERE degrees >= 5
  ORDER BY degrees DESC
  LIMIT 20;
```

### Deployment on Google Cloud Run

Each time a new commit is pushed to `main`, Google Cloud Build will automatically build a container containing the both the server and tool, which are tagged with the commit hash. However, this will be without the local graph data files, and therefore cannot be deployed directly to Cloud Run. The container will need the data files baked into it. The container for the Cloud Run service will also need to be updated every time Wikipedia releases a new database dump.

Wikipedia's dump server seems to throttle dump requests, so it's advantageous to download the dumps and then upload them to GCS. They are currently pushed to GCS but not used during the service-image build process.

#### cloudbuild-service.yaml

Invoking this build will:

* Find the latest dump date from the Wikipedia database dumps
* Find the container that is running the Cloud Run service, and extract the dump date from its tag
* If the Wikipedia dump date is newer than the Cloud Run container's dump date, it will start a new Cloud Build job to build a new `wikiwalk-service` image (see `Dockerfile.service`) - This will find the latest date that the Wikipedia database dumps were updated, and if that date is newer than the date of the latest graph database, it will download the new dumps, build a new graph database, and upload all the artifacts to Google Cloud Storage.


#### Triggers:

* A new commit is pushed to `main`
  * Google Cloud Build will build a new "app" container (containing `/server` and `/tool`) and push it to the container registry
  * A new Cloud Run Job will run `tool sync-cloud-data`
* Every day the cloudbuild-service.yaml should be run
